{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f359cfd3-ecbe-4e76-83e1-24925a03268f",
      "metadata": {
        "scrolled": true,
        "id": "f359cfd3-ecbe-4e76-83e1-24925a03268f",
        "outputId": "4c16ac7b-1463-4e2d-ea7c-64a9d98d4a7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 819
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduction — NVIDIA Generative AI Workflow Skip to main content Back to top Ctrl + K NVIDIA Generative AI Workflow Documentation Home Is this page helpful? Introduction # Modern enterprise applications are becoming more cloud-native and based on a microservices architecture. Microservices, by definition, consist of a collection of small independent services that communicate over well-defined APIs. AI applications, in most instances, adhere well to this same architectural design, as there are typically many different components that all need to work together in both training and inferencing workflows. To deploy an application in a production environment, the application must also meet the following criteria: Reliability Security Performance Scalability Interoperability What are NVIDIA AI Workflows? # NVIDIA AI Workflows are intended to provide reference solutions of how to leverage NVIDIA frameworks to build AI solutions for solving common use cases. These workflows guide fine-tuning and AI model creation to build upon NVIDIA frameworks. The pipelines to create applications are highlighted, as well as opinions on how to deploy customized applications and integrate them with various components typically found in enterprise environments, such as components for orchestration and management, storage, security, networking, etc. By leveraging an AI workflow for your specific use case, you can streamline the development of AI solutions following the example provided by the workflow to: Reduce development time, at a lower cost Improve accuracy and performance Gain confidence in the outcome, by leveraging NVIDIA AI expertise Using the example workflow, you know exactly what AI framework to use, how to bring data into the pipeline, and what to do with the data output. AI Workflows are designed as microservices, which means they can be deployed on Kubernetes alone or with other microservices to create a production-ready application for seamless scaling. The workflow cloud deployable package can be used across different cloud instances and is automatable and interoperable. NVIDIA AI Workflows are available on NVIDIA NGC for NVIDIA AI Enterprise software customers. On this page\n",
            "###########################\n",
            "0: Introduction — NVIDIA Generative AI Workflow Skip to main content Back to top Ctrl + K NVIDIA Generative AI Workflow Documentation Home Is this page helpful?\n",
            "1: Introduction # Modern enterprise applications are becoming more cloud-native and based on a microservices architecture.\n",
            "2: Microservices, by definition, consist of a collection of small independent services that communicate over well-defined APIs.\n",
            "3: AI applications, in most instances, adhere well to this same architectural design, as there are typically many different components that all need to work together in both training and inferencing workflows.\n",
            "4: To deploy an application in a production environment, the application must also meet the following criteria: Reliability Security Performance Scalability Interoperability What are NVIDIA AI Workflows?\n",
            "5: # NVIDIA AI Workflows are intended to provide reference solutions of how to leverage NVIDIA frameworks to build AI solutions for solving common use cases.\n",
            "6: These workflows guide fine-tuning and AI model creation to build upon NVIDIA frameworks.\n",
            "7: The pipelines to create applications are highlighted, as well as opinions on how to deploy customized applications and integrate them with various components typically found in enterprise environments, such as components for orchestration and management, storage, security, networking, etc.\n",
            "8: By leveraging an AI workflow for your specific use case, you can streamline the development of AI solutions following the example provided by the workflow to: Reduce development time, at a lower cost Improve accuracy and performance Gain confidence in the outcome, by leveraging NVIDIA AI expertise Using the example workflow, you know exactly what AI framework to use, how to bring data into the pipeline, and what to do with the data output.\n",
            "9: AI Workflows are designed as microservices, which means they can be deployed on Kubernetes alone or with other microservices to create a production-ready application for seamless scaling.\n",
            "--------------------------------- xxxx ----------------------------------\n",
            "Creating embeddings\n",
            "DONE Creating embeddings\n",
            "Shape of embeddings: 13 384\n",
            "384\n",
            "Created Index\n",
            "ASk a question ?: what is this document about?\n",
            "what is this document about?\n",
            "Initiating search\n",
            "Match: On this page\n",
            "Match: Introduction — NVIDIA Generative AI Workflow Skip to main content Back to top Ctrl + K NVIDIA Generative AI Workflow Documentation Home Is this page helpful?\n",
            "Match: The pipelines to create applications are highlighted, as well as opinions on how to deploy customized applications and integrate them with various components typically found in enterprise environments, such as components for orchestration and management, storage, security, networking, etc.\n",
            "ASk a question ?: who should use this document?\n",
            "who should use this document?\n",
            "Initiating search\n",
            "Match: On this page\n",
            "Match: The pipelines to create applications are highlighted, as well as opinions on how to deploy customized applications and integrate them with various components typically found in enterprise environments, such as components for orchestration and management, storage, security, networking, etc.\n",
            "Match: # NVIDIA AI Workflows are intended to provide reference solutions of how to leverage NVIDIA frameworks to build AI solutions for solving common use cases.\n",
            "ASk a question ?: how do I install the software?\n",
            "how do I install the software?\n",
            "Initiating search\n",
            "Match: On this page\n",
            "Match: NVIDIA AI Workflows are available on NVIDIA NGC for NVIDIA AI Enterprise software customers.\n",
            "Match: These workflows guide fine-tuning and AI model creation to build upon NVIDIA frameworks.\n",
            "ASk a question ?: bye\n",
            "bye\n",
            "Good Bye! Thanks for using my documentation Analyzer. See you soon\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n        D, I = index.search(np.array(embeddings), k=1)\\n        print(len(sentences))\\n        for s in sentences:\\n          print(s)\\n          print(\"---------------\")\\n        print(\"Most similar sentence:\\n\", sentences[I[0][0]])\\n        print(\"---------------------------------------------\")\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import faiss\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Step 1: URL of the page you want to read\n",
        "url = \"https://docs.nvidia.com/ai-enterprise/workflow/generative-ai/latest/introduction.html\"\n",
        "\n",
        "# Step 2: Fetch the HTML content\n",
        "response = requests.get(url)\n",
        "# Check for success\n",
        "if response.status_code == 200:\n",
        "    html = response.text\n",
        "\n",
        "    # Step 3: Parse with BeautifulSoup\n",
        "    soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "    # Step 4: Remove scripts and style elements\n",
        "    for tag in soup([\"script\", \"style\", \"nav\", \"header\", \"footer\", \"aside\"]):\n",
        "        tag.decompose()\n",
        "    # Step 5: Extract plain text\n",
        "    text = soup.get_text(separator=' ', strip=True)\n",
        "    print(text)\n",
        "    print(\"###########################\")\n",
        "else:\n",
        "    print(f\"Failed to fetch page: {response.status_code}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "for i, sent in enumerate(sentences[:10]):\n",
        "    print(f\"{i}: {sent}\")\n",
        "print(\"--------------------------------- xxxx ----------------------------------\")\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "print(\"Creating embeddings\")\n",
        "embeddings = model.encode(sentences)\n",
        "print(\"DONE Creating embeddings\")\n",
        "print(\"Shape of embeddings:\", len(embeddings), len(embeddings[0]))\n",
        "dim = len(embeddings[0])\n",
        "print(dim)\n",
        "# Build FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])  # 384 = vector dimension of MiniLM\n",
        "index.add(np.array(embeddings))\n",
        "print(\"Created Index\")\n",
        "while True:\n",
        "    query = input(\"ASk a question ?: \")\n",
        "    print(query)\n",
        "    if query.lower() == \"bye\":\n",
        "        print(\"Good Bye! Thanks for using my documentation Analyzer. See you soon\")\n",
        "        break\n",
        "    else :\n",
        "        print(\"Initiating search\")\n",
        "        query_emb = model.encode([query])\n",
        "        D, I = index.search(np.array(query_emb), k=3)\n",
        "        for idx in I[0]:\n",
        "          print(\"Match:\", sentences[idx])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "uAZpxHtRllce",
        "outputId": "b17c0e8e-0048-445a-9e74-4239c071523c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uAZpxHtRllce",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ie8v9L_lmBJ"
      },
      "id": "0ie8v9L_lmBJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17f3c7b1-e4ae-4b1e-9509-5485d53b0781",
      "metadata": {
        "id": "17f3c7b1-e4ae-4b1e-9509-5485d53b0781"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (ai-playground)",
      "language": "python",
      "name": "ai-playground"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}